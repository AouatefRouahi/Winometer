{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46836482-d506-4507-b03d-6bdaf01154ab",
   "metadata": {},
   "source": [
    "# Wine-o-meter : Classification\n",
    "-----\n",
    "> The prediction problem can be handled either as a classification task or as a regression task. In the current proposal, we will use classification techniques.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21facb0-5888-4543-af7a-3cef2c13107d",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Data Preparation](#section1)\n",
    "    * [1.1. Load Data](#section21)\n",
    "    * [1.2. Predictors and Target](#section21)\n",
    "    * [1.3. Training and Validation sets](#section22)\n",
    "    * [1.4. Preprocessing pipeline](#section22)\n",
    "* [2. Classification](#section22)\n",
    "    * [2.1. Preliminary Model Selection](#section21)\n",
    "    * [2.2. RandomForest Classifier](#section21)\n",
    "    * [2.3. Support Vector Classifier (SVC)](#section22)\n",
    "    * [2.4. Save the selected model](#section22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb0c7de-ca6b-4f81-b147-ab6fce8dfb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic libs\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Ml libs\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# predefined modules\n",
    "from modules import MyFunctions as MyFunct\n",
    "\n",
    "file_path = \"data/winequality.csv\"\n",
    "model_path= 'models/model.joblib'\n",
    "\n",
    "seed = 0\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "scoring = 'accuracy'\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a81226-710a-40a5-9734-887fb33ba7c3",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31bc9b-57e0-4d55-96af-d25b9923afde",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc87a888-3e6e-47bb-a899-2886fce4f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cdd0a-9fd7-433f-9776-be039335b638",
   "metadata": {},
   "source": [
    "## Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9702ee24-b73c-4bf8-831d-8cc4e2b5e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the predictors and the target\n",
    "y = df[\"quality\"]\n",
    "X = df.drop([\"quality\", \"type\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2f816-615b-457e-8bdc-de80bedd56f6",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f95100-ccc9-453a-8e8f-c8ef421ed6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = MyFunct.train_val(X, y, train_ratio, val_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc21dd0-a681-4a6c-b09a-b7b4e61acfc9",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47579029-96ee-42cd-9f3a-9024c036159a",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\"><ol>\n",
    "<li><b>Missing values imputation </b>: we will use the median to fill in the missing values. The median is the safe way for data imputation because if the data distribution is skewed the mean is biased by outliers.\n",
    "<li><b>Standardization</b>: we will standardize the numerical data before training to eliminate large scales effect on the learning phase.</ol>\n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca6e080-6867-4eba-b3db-e5a7db1149cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(steps = [(\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "                                 (\"scaler\", StandardScaler())])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_val = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ef58e-f12f-47a6-8058-d352f94b0530",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4988d-1152-43c6-95ca-f954a1820cd3",
   "metadata": {},
   "source": [
    "## Preliminary Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c3fe-7979-4788-bb40-3c7f9fa942af",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\">\n",
    "In this part, we want to establish a preliminary performance evaluation to get some first insights on the classification techniques that can be efficiently used to solve the current prediction problem. We will evaluate the baseline performance of various techniques, using the default settings as proposed by the ML library <b>sklearn</b>, by means of the <b>k-fold cross validation</b> technique.  \n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863920d9-a9d7-4091-9a87-68b4b11e816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LogisticRegression is done in 2.9933881759643555s\n",
      "fitting SVC is done in 2.6053969860076904s\n",
      "fitting GaussianNB is done in 0.023194313049316406s\n",
      "fitting RandomForestClassifier is done in 1.6209118366241455s\n",
      "fitting AdaBoostClassifier is done in 0.5535402297973633s\n",
      "fitting GradientBoostingClassifier is done in 10.447529792785645s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.647104</td>\n",
       "      <td>0.006674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.576489</td>\n",
       "      <td>0.010655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.560324</td>\n",
       "      <td>0.003719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.536657</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.453719</td>\n",
       "      <td>0.040946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.428707</td>\n",
       "      <td>0.005776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name      mean       std\n",
       "3      RandomForestClassifier  0.647104  0.006674\n",
       "5  GradientBoostingClassifier  0.576489  0.010655\n",
       "1                         SVC  0.560324  0.003719\n",
       "0          LogisticRegression  0.536657  0.004098\n",
       "4          AdaBoostClassifier  0.453719  0.040946\n",
       "2                  GaussianNB  0.428707  0.005776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(max_iter=500),\n",
    "    SVC(),\n",
    "    GaussianNB(),\n",
    "    RandomForestClassifier(random_state = seed),\n",
    "    AdaBoostClassifier(random_state = seed),\n",
    "    GradientBoostingClassifier(random_state = seed)\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for clf in classifiers:\n",
    "    scores.append(MyFunct.model_validation(clf, X_train, y_train, cv, scoring))\n",
    "    \n",
    "scores_df = pd.DataFrame(scores, columns= ['name', 'mean', 'std'])\n",
    "scores_df.sort_values(by=['mean','std'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f7fad-3f4b-41b0-bd0c-cb819242fbaf",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\">\n",
    "<li>The multiclass classification problem is not well handled with neither the <b>Naive Bayes</b> classifier nor the <b>AdaBoost</b> one. Other poor performer is the <b>logisticRegression</b> model. The <b>GradientBoosting</b> and the <b>SVC</b> classifiers have very close means. However, the std of the scores got from the <b>SVC</b> is way lower than the std got from <b>GradienBoosting</b>.\n",
    "\n",
    "<li>Given this preliminary performance analysis, we want to further check the <b>RandomForest</b> classifier that gives the best scores and the <b>SVC</b> using <b>hyperparmeters tuning</b> by the means of the <b>GridSearchCV</b> technique. \n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2af595-7a9c-40fe-a8fe-9c1d0cd07fab",
   "metadata": {},
   "source": [
    "## RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf3261-d83c-4a61-9f22-be95ee126b93",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\">\n",
    "We will tune the most important hyperparameters that are: \n",
    "<li>the used splitting criterion, \n",
    "<li>the number of estimators, \n",
    "<li>the maximum depth of the tree estimators, \n",
    "<li>the minimum number of samples to make a split, \n",
    "<li>the minimum number of samples to generate leaves and finally, \n",
    "<li>the maximum number of features to be taken into account to select best splitters.\n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829beb5c-1536-436b-8f97-083c2183f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv =  PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0]))\n",
      "Fitting 1 folds for each of 810 candidates, totalling 810 fits\n",
      "Tuning RandomForestClassifier hyperparameters is done in 1031.071202993393s\n",
      "\n",
      "Best Estimator \n",
      "\n",
      "Best Params \n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'max_features': 8, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best score \n",
      "\n",
      "0.6715384615384615\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators' : [50, 100, 200],\n",
    "    'max_depth' : [10, 15, 20],\n",
    "    'min_samples_split' : [5, 10, 20],\n",
    "    'min_samples_leaf' : [3, 5, 10],\n",
    "    'max_features': [2, 4, 6, 8, 11]\n",
    "}\n",
    "rf_classifier = MyFunct.model_selection(RandomForestClassifier(random_state = seed), X_train, y_train, X_val, y_val, params, scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65f707-5451-40e5-a4c9-4fd6c6430e49",
   "metadata": {},
   "source": [
    "## Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a625d-78dc-4eb7-8b03-4173aff2cffc",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\">\n",
    "We will tune the most important hyperparameters that are: \n",
    "<li>the kernel function, \n",
    "<li>the regularization parameter C and \n",
    "<li>the gamma parameter that is used to reduce the prediction sensitivity to individual samples. Note that gamma is a parameter that is used with only some specific kernel functions. \n",
    "\n",
    "We will check 2 kernel functions that are the <b>linear</b> and the <b>rbf</b> kernels.\n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928dde2d-044f-4239-a369-39b009b137d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv =  PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0]))\n",
      "Fitting 1 folds for each of 8 candidates, totalling 8 fits\n",
      "Tuning SVC hyperparameters is done in 1074.62743973732s\n",
      "\n",
      "Best Estimator \n",
      "\n",
      "Best Params \n",
      "\n",
      "{'C': 0.1, 'class_weight': None}\n",
      "Best score \n",
      "\n",
      "0.5361538461538462\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [0.1, 10.0, 50.0, 100.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "sv_classifier = MyFunct.model_selection(SVC(kernel = 'linear', random_state= seed), X_train, y_train, X_val, y_val, params, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64aa8907-38ac-4282-9707-a41a18791a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv =  PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0]))\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "Tuning SVC hyperparameters is done in 24.949832439422607s\n",
      "\n",
      "Best Estimator \n",
      "\n",
      "Best Params \n",
      "\n",
      "{'C': 10.0, 'gamma': 1.0}\n",
      "Best score \n",
      "\n",
      "0.6523076923076923\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'gamma': [0.01, 0.1, 1.0]\n",
    "}\n",
    "sv_classifier = MyFunct.model_selection(SVC(kernel = 'rbf', random_state= seed), X_train, y_train, X_val, y_val, params, scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6ead3-ac0b-4d98-9b9a-29d00499ec05",
   "metadata": {},
   "source": [
    "<pre>\n",
    "üìù <b>Note</b>\n",
    "<div style=\"background-color:#C2F2ED;\">\n",
    "<li>With the hyperparameters tuning we succeeded to increase the accuracy scores with some points.\n",
    "\n",
    "<li>Clearly, the <b>RandomForest</b> classifier is the most suitable (among the checked algorithms) for this prediction problem. Hence, we will use it to make the final predictions. We will save it as a <b>joblib</b> file to be reused later.\n",
    "</div> </pre> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816efedb-76cb-4f34-98e5-84e9701503c7",
   "metadata": {},
   "source": [
    "## Save the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18ea442-ca70-42bd-a4b4-ea85e231989e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=15, max_features=8,\n",
       "                       min_samples_leaf=3, min_samples_split=5,\n",
       "                       n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726ffe9a-14b4-4dde-9123-bca49efc6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(rf_classifier.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1a61ea-5ed5-4b0b-a98f-b5ee0c529319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_classifier, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
